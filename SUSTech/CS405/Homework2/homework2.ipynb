{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast = load_breast_cancer()\n",
    "\n",
    "X = breast['data']\n",
    "y = breast['target']\n",
    "\n",
    "np.random.seed(100)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_val, y_val = X[400:500], y[400:500]\n",
    "X_test, y_test = X[500:], y[500:]\n",
    "\n",
    "\n",
    "def distanceFunc(metric_type, vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the distance between two d-dimension vectors. \n",
    "    \n",
    "    Please DO NOT use Numpy's norm function when implementing this function. \n",
    "    \n",
    "    Args:\n",
    "        metric_type (str): Metric: L1, L2, or L-inf\n",
    "        vec1 ((d,) np.ndarray): d-dim vector\n",
    "        vec2 ((d,)) np.ndarray): d-dim vector\n",
    "    \n",
    "    Returns:\n",
    "        distance (float): distance between the two vectors\n",
    "    \"\"\"\n",
    "\n",
    "    diff = vec1 - vec2\n",
    "    if metric_type == \"L1\":\n",
    "        return np.sum([np.abs(i) for i in diff])\n",
    "\n",
    "    if metric_type == \"L2\":\n",
    "        return np.sqrt(np.sum([i**2 for i in diff]))\n",
    "        \n",
    "    if metric_type == \"L-inf\":\n",
    "        return np.max([np.abs(i) for i in diff])\n",
    "\n",
    "\n",
    "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
    "    \"\"\"\n",
    "    Compute the distances between every datapoint in the train_data and the \n",
    "    given sample. Then, find the k-nearest neighbors.\n",
    "    \n",
    "    Return a numpy array of the label of the k-nearest neighbors.\n",
    "    \n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        sample ((p,) np.ndarray): Single sample whose distance is to computed with every entry in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "    \"\"\"\n",
    "\n",
    "    dis = [distanceFunc(metric_type, sample, X_train[i, :]) for i in range(X_train.shape[0])]\n",
    "    idx = sorted(range(X_train.shape[0]), key=dis.__getitem__)\n",
    "    neighbors = [y_train[idx[i]] for i in range(K)]\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def Majority(neighbors):\n",
    "    \"\"\"\n",
    "    Performs majority voting and returns the predicted value for the test sample.\n",
    "    \n",
    "    Since we're performing binary classification the possible values are [0,1].\n",
    "    \n",
    "    Args:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "        \n",
    "    Returns:\n",
    "        predicted_value (int): predicted label for the given sample\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = [neighbors.count(i) for i in neighbors]\n",
    "    predicted_value = neighbors[np.argmax(counts)]\n",
    "\n",
    "    return predicted_value\n",
    "\n",
    "\n",
    "def KNN(K, metric_type, X_train, y_train, X_val):\n",
    "    \"\"\"\n",
    "    Returns the predicted values for the entire validation or test set.\n",
    "    \n",
    "    Please DO NOT use Scikit's KNN model when implementing this function. \n",
    "\n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        X_val ((n, p) np.ndarray): Validation or test data\n",
    "        \n",
    "    Returns:\n",
    "        predicted_values (list): output for every entry in validation/test dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(X_val.shape[0]):\n",
    "        sample = X_val[i, :]\n",
    "        neighbors = computeDistancesNeighbors(K, metric_type, X_train, y_train, sample)\n",
    "\n",
    "        predictions.append(Majority(neighbors))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluation(predicted_values, actual_values):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the given datapoints.\n",
    "    \n",
    "    Args:\n",
    "        predicted_values ((n,) np.ndarray): Predicted values for n samples\n",
    "        actual_values ((n,) np.ndarray): Actual values for n samples\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    return accuracy_score(predicted_values, actual_values)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Calls the above functions in order to implement the KNN algorithm.\n",
    "    \n",
    "    Test over the following range K = 3,5,7 and all three metrics. \n",
    "    In total you will have nine combinations to try.\n",
    "    \n",
    "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
    "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
    "    \n",
    "    REMEMBER: You have to report these values by populating the Table 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Complete this function\n",
    "    \n",
    "    K = [3, 5, 7]\n",
    "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
    "    \n",
    "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
    "\n",
    "    values = []\n",
    "    for i in K:\n",
    "        print(f'for K = {i}:')\n",
    "        print(f'\\tL1\\tL2\\tL-inf')\n",
    "\n",
    "        values = []\n",
    "        predictions = KNN(i, \"L1\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions, y_val)\n",
    "        values.append(acc)\n",
    "        \n",
    "        predictions = KNN(i, \"L2\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions, y_val)\n",
    "        values.append(acc)\n",
    "\n",
    "        predictions = KNN(i, \"L-inf\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions, y_val)\n",
    "        values.append(acc)\n",
    "        print(f'acc\\t{values[0]}\\t{values[1]}\\t{values[2]}')\n",
    "\n",
    "    print(\"\\n<<<<TEST DATA PREDICTIONS>>>>\")\n",
    "\n",
    "    values.clear()\n",
    "    for i in K:\n",
    "        print(f'for K = {i}:')\n",
    "        print(f'\\tL1\\tL2\\tL-inf')\n",
    "\n",
    "        predictions = KNN(i, \"L1\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions, y_test)\n",
    "        values.append(acc)\n",
    "        \n",
    "        predictions = KNN(i, \"L2\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions, y_test)\n",
    "        values.append(acc)\n",
    "\n",
    "        predictions = KNN(i, \"L-inf\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions, y_test)\n",
    "        values.append(acc)\n",
    "        print(f'acc\\t{values[0]}\\t{values[1]}\\t{values[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<<<<VALIDATION DATA PREDICTIONS>>>>\n",
      "for K = 3:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.94\t0.95\t0.94\n",
      "for K = 5:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.94\t0.93\t0.94\n",
      "for K = 7:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.93\t0.92\t0.93\n",
      "\n",
      "<<<<TEST DATA PREDICTIONS>>>>\n",
      "for K = 3:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.8840579710144928\t0.8840579710144928\t0.8985507246376812\n",
      "for K = 5:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.8840579710144928\t0.8840579710144928\t0.8985507246376812\n",
      "for K = 7:\n",
      "\tL1\tL2\tL-inf\n",
      "acc\t0.8840579710144928\t0.8840579710144928\t0.8985507246376812\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions here:\n",
    "\n",
    "1. How could having a larger dataset influence the performance of KNN?\n",
    "\n",
    "2. Tabulate your results from `main()` in the table provided.\n",
    "\n",
    "3. Finally, mention the best K and the norm combination you have settled upon and report the accuracy on the test set using that combination."
   ]
  },
  {
   "source": [
    "1. KNN would have a better performance since a greate deviation may exist when the dataset is small. Coincidence can influence the results a lot in small datasets.\n",
    "\n",
    "2. The result is shown above.\n",
    "\n",
    "3. The best combination of my result is {K=3, L2}. The accuracy is 0.95."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}