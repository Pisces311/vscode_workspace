{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-ProgramQuestion\n",
    "1. H = W = (63 - 7 + 2 * 0) / 2 + 1 = 29. The output volume is 29 * 29 * 32\n",
    "\n",
    "2. 300 * 300 * 3 * 100 + 1 = 27000001 parameters\n",
    "\n",
    "3. convolved with filter 1:\n",
    "\n",
    "| 9    | 8    | 2    |\n",
    "| ---- | ---- | ---- |\n",
    "| 4    | 16   | 9    |\n",
    "| 16   | 22   | 3    |\n",
    "\n",
    "convolved with filter 2:\n",
    "\n",
    "| 20   | 8    | -6   |\n",
    "| ---- | ---- | ---- |\n",
    "| 7    | -1   | 19   |\n",
    "| 24   | 7    | 0    |\n",
    "\n",
    "4. 30 * 20 + 1 + (20 * 20 + 1) * 8 + 20 + 1 = 3830\n",
    "\n",
    "5. CNN has smaller number of parameters to be learned. CNN utilzes the location information of the pixels in the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAqXcDzKwj4D"
   },
   "source": [
    "# HW5_programQuestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import libraries that you might require\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvDAZByo_X1w"
   },
   "source": [
    "## 1. Loading the Dataset\n",
    "The output of torchvision datasets are PILImage images of range [0, 1].  \n",
    "We transform them to Tensors of normalized range [-1, 1].  \n",
    "```Transforms.Normalize((mean,),(std,))``` basically manipulates the values of a pixel such that  \n",
    "$$New\\_Value = \\frac{Old\\_Value - Mean}{Std}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "\n",
    "#TODO : Set the value of mean and the standard deviation to \n",
    "#       normalize the image from range [0,1] to the range [-1, 1]\n",
    "\n",
    "\n",
    "#Begin Your Code\n",
    "\n",
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "\n",
    "#End Your Code\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)\n",
    "                                ])\n",
    "\n",
    "\n",
    "#TODO : Select suitable value of batch_sizes.\n",
    "\n",
    "#Begin Your Code\n",
    "\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    " \n",
    "#End Your Code\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# Classes\n",
    "classes = {       0 :'T-shirt/top',\n",
    "                  1 :'Trouser',\n",
    "                  2 :'Pullover',\n",
    "                  3 :'Dress',\n",
    "                  4 :'Coat',\n",
    "                  5 :'Sandal',\n",
    "                  6 :'Shirt',\n",
    "                  7 :'Sneaker',\n",
    "                  8 :'Bag',\n",
    "                  9 :'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFBh6ZhO_X1y"
   },
   "source": [
    "## 2. The Dataset\n",
    "Here we show some images of the dataset.  \n",
    "See how many of the categories can you recognise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "# Functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le9N91sO_X11"
   },
   "source": [
    "## 3. Create your Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #TODO : Design your network, you are allowed to explore your own architecture\n",
    "        #       But you should achieve a better overall accuracy than the baseline network.\n",
    "        #       Also, if you do design your own network, include an explanation \n",
    "        #       for your choice of network and how it may be better than the \n",
    "        #       baseline network.\n",
    "        \n",
    "        #Begin Your Code\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 520)\n",
    "        self.fc2 = nn.Linear(520, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        #End Your Code\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #TODO : Implement the forward function that applies the layers you have created to the input\n",
    "\n",
    "        #Begin Your Code\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 7 * 7 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        #End Your Code\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jieF6xy__X13"
   },
   "source": [
    "## 4. Define a Loss function and optimizer\n",
    "We will be using [Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) and [Adam optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam).  \n",
    "Note: PyTorch's CrossEntropyLoss combines log softmax and negative log likelihood loss in one class. Make sure you are not computing softmax twice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#TODO : Use appropriate loss criterion and optimizer \n",
    "\n",
    "#Begin Your Code\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "#End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4za3t0b_X15"
   },
   "source": [
    "## 5. Train the network\n",
    "\n",
    "Here we are going to train the network while logging the per batch metrics.  \n",
    "This would take some time to run (5-10 minutes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch:   200, loss: 1.523\n",
      "Epoch: 1, Batch:   400, loss: 0.893\n",
      "Epoch: 1, Batch:   600, loss: 0.816\n",
      "Epoch: 1, Batch:   800, loss: 0.759\n",
      "Epoch: 1, Batch:  1000, loss: 0.754\n",
      "Epoch: 1, Batch:  1200, loss: 0.669\n",
      "Epoch: 1, Batch:  1400, loss: 0.716\n",
      "Epoch: 1, Batch:  1600, loss: 0.682\n",
      "Epoch: 1, Batch:  1800, loss: 0.627\n",
      "Epoch: 1, Batch:  2000, loss: 0.634\n",
      "Epoch: 1, Batch:  2200, loss: 0.630\n",
      "Epoch: 1, Batch:  2400, loss: 0.561\n",
      "Epoch: 1, Batch:  2600, loss: 0.558\n",
      "Epoch: 1, Batch:  2800, loss: 0.569\n",
      "Epoch: 1, Batch:  3000, loss: 0.524\n",
      "Epoch: 1, Batch:  3200, loss: 0.546\n",
      "Epoch: 1, Batch:  3400, loss: 0.513\n",
      "Epoch: 1, Batch:  3600, loss: 0.532\n",
      "Epoch: 1, Batch:  3800, loss: 0.509\n",
      "Epoch: 1, Batch:  4000, loss: 0.502\n",
      "Epoch: 1, Batch:  4200, loss: 0.547\n",
      "Epoch: 1, Batch:  4400, loss: 0.527\n",
      "Epoch: 1, Batch:  4600, loss: 0.474\n",
      "Epoch: 1, Batch:  4800, loss: 0.532\n",
      "Epoch: 1, Batch:  5000, loss: 0.512\n",
      "Epoch: 1, Batch:  5200, loss: 0.469\n",
      "Epoch: 1, Batch:  5400, loss: 0.546\n",
      "Epoch: 1, Batch:  5600, loss: 0.558\n",
      "Epoch: 1, Batch:  5800, loss: 0.492\n",
      "Epoch: 1, Batch:  6000, loss: 0.486\n",
      "Epoch: 1, Batch:  6200, loss: 0.493\n",
      "Epoch: 1, Batch:  6400, loss: 0.459\n",
      "Epoch: 1, Batch:  6600, loss: 0.445\n",
      "Epoch: 1, Batch:  6800, loss: 0.476\n",
      "Epoch: 1, Batch:  7000, loss: 0.466\n",
      "Epoch: 1, Batch:  7200, loss: 0.439\n",
      "Epoch: 1, Batch:  7400, loss: 0.504\n",
      "Epoch: 1, Batch:  7600, loss: 0.557\n",
      "Epoch: 1, Batch:  7800, loss: 0.414\n",
      "Epoch: 1, Batch:  8000, loss: 0.416\n",
      "Epoch: 1, Batch:  8200, loss: 0.427\n",
      "Epoch: 1, Batch:  8400, loss: 0.467\n",
      "Epoch: 1, Batch:  8600, loss: 0.450\n",
      "Epoch: 1, Batch:  8800, loss: 0.459\n",
      "Epoch: 1, Batch:  9000, loss: 0.500\n",
      "Epoch: 1, Batch:  9200, loss: 0.450\n",
      "Epoch: 1, Batch:  9400, loss: 0.450\n",
      "Epoch: 1, Batch:  9600, loss: 0.441\n",
      "Epoch: 1, Batch:  9800, loss: 0.381\n",
      "Epoch: 1, Batch: 10000, loss: 0.425\n",
      "Epoch: 1, Batch: 10200, loss: 0.469\n",
      "Epoch: 1, Batch: 10400, loss: 0.430\n",
      "Epoch: 1, Batch: 10600, loss: 0.451\n",
      "Epoch: 1, Batch: 10800, loss: 0.403\n",
      "Epoch: 1, Batch: 11000, loss: 0.469\n",
      "Epoch: 1, Batch: 11200, loss: 0.450\n",
      "Epoch: 1, Batch: 11400, loss: 0.414\n",
      "Epoch: 1, Batch: 11600, loss: 0.445\n",
      "Epoch: 1, Batch: 11800, loss: 0.394\n",
      "Epoch: 1, Batch: 12000, loss: 0.434\n",
      "Epoch: 1, Batch: 12200, loss: 0.408\n",
      "Epoch: 1, Batch: 12400, loss: 0.389\n",
      "Epoch: 1, Batch: 12600, loss: 0.428\n",
      "Epoch: 1, Batch: 12800, loss: 0.379\n",
      "Epoch: 1, Batch: 13000, loss: 0.383\n",
      "Epoch: 1, Batch: 13200, loss: 0.444\n",
      "Epoch: 1, Batch: 13400, loss: 0.405\n",
      "Epoch: 1, Batch: 13600, loss: 0.371\n",
      "Epoch: 1, Batch: 13800, loss: 0.408\n",
      "Epoch: 1, Batch: 14000, loss: 0.440\n",
      "Epoch: 1, Batch: 14200, loss: 0.434\n",
      "Epoch: 1, Batch: 14400, loss: 0.392\n",
      "Epoch: 1, Batch: 14600, loss: 0.444\n",
      "Epoch: 1, Batch: 14800, loss: 0.322\n",
      "Epoch: 1, Batch: 15000, loss: 0.446\n",
      "Epoch: 2, Batch:   200, loss: 0.417\n",
      "Epoch: 2, Batch:   400, loss: 0.473\n",
      "Epoch: 2, Batch:   600, loss: 0.426\n",
      "Epoch: 2, Batch:   800, loss: 0.376\n",
      "Epoch: 2, Batch:  1000, loss: 0.412\n",
      "Epoch: 2, Batch:  1200, loss: 0.414\n",
      "Epoch: 2, Batch:  1400, loss: 0.385\n",
      "Epoch: 2, Batch:  1600, loss: 0.422\n",
      "Epoch: 2, Batch:  1800, loss: 0.339\n",
      "Epoch: 2, Batch:  2000, loss: 0.416\n",
      "Epoch: 2, Batch:  2200, loss: 0.398\n",
      "Epoch: 2, Batch:  2400, loss: 0.419\n",
      "Epoch: 2, Batch:  2600, loss: 0.324\n",
      "Epoch: 2, Batch:  2800, loss: 0.350\n",
      "Epoch: 2, Batch:  3000, loss: 0.423\n",
      "Epoch: 2, Batch:  3200, loss: 0.391\n",
      "Epoch: 2, Batch:  3400, loss: 0.348\n",
      "Epoch: 2, Batch:  3600, loss: 0.332\n",
      "Epoch: 2, Batch:  3800, loss: 0.373\n",
      "Epoch: 2, Batch:  4000, loss: 0.417\n",
      "Epoch: 2, Batch:  4200, loss: 0.369\n",
      "Epoch: 2, Batch:  4400, loss: 0.353\n",
      "Epoch: 2, Batch:  4600, loss: 0.388\n",
      "Epoch: 2, Batch:  4800, loss: 0.412\n",
      "Epoch: 2, Batch:  5000, loss: 0.434\n",
      "Epoch: 2, Batch:  5200, loss: 0.372\n",
      "Epoch: 2, Batch:  5400, loss: 0.367\n",
      "Epoch: 2, Batch:  5600, loss: 0.406\n",
      "Epoch: 2, Batch:  5800, loss: 0.364\n",
      "Epoch: 2, Batch:  6000, loss: 0.442\n",
      "Epoch: 2, Batch:  6200, loss: 0.348\n",
      "Epoch: 2, Batch:  6400, loss: 0.343\n",
      "Epoch: 2, Batch:  6600, loss: 0.306\n",
      "Epoch: 2, Batch:  6800, loss: 0.326\n",
      "Epoch: 2, Batch:  7000, loss: 0.337\n",
      "Epoch: 2, Batch:  7200, loss: 0.404\n",
      "Epoch: 2, Batch:  7400, loss: 0.403\n",
      "Epoch: 2, Batch:  7600, loss: 0.368\n",
      "Epoch: 2, Batch:  7800, loss: 0.362\n",
      "Epoch: 2, Batch:  8000, loss: 0.398\n",
      "Epoch: 2, Batch:  8200, loss: 0.428\n",
      "Epoch: 2, Batch:  8400, loss: 0.362\n",
      "Epoch: 2, Batch:  8600, loss: 0.384\n",
      "Epoch: 2, Batch:  8800, loss: 0.374\n",
      "Epoch: 2, Batch:  9000, loss: 0.326\n",
      "Epoch: 2, Batch:  9200, loss: 0.349\n",
      "Epoch: 2, Batch:  9400, loss: 0.343\n",
      "Epoch: 2, Batch:  9600, loss: 0.389\n",
      "Epoch: 2, Batch:  9800, loss: 0.407\n",
      "Epoch: 2, Batch: 10000, loss: 0.329\n",
      "Epoch: 2, Batch: 10200, loss: 0.446\n",
      "Epoch: 2, Batch: 10400, loss: 0.291\n",
      "Epoch: 2, Batch: 10600, loss: 0.382\n",
      "Epoch: 2, Batch: 10800, loss: 0.323\n",
      "Epoch: 2, Batch: 11000, loss: 0.396\n",
      "Epoch: 2, Batch: 11200, loss: 0.342\n",
      "Epoch: 2, Batch: 11400, loss: 0.363\n",
      "Epoch: 2, Batch: 11600, loss: 0.357\n",
      "Epoch: 2, Batch: 11800, loss: 0.371\n",
      "Epoch: 2, Batch: 12000, loss: 0.417\n",
      "Epoch: 2, Batch: 12200, loss: 0.399\n",
      "Epoch: 2, Batch: 12400, loss: 0.388\n",
      "Epoch: 2, Batch: 12600, loss: 0.377\n",
      "Epoch: 2, Batch: 12800, loss: 0.333\n",
      "Epoch: 2, Batch: 13000, loss: 0.346\n",
      "Epoch: 2, Batch: 13200, loss: 0.385\n",
      "Epoch: 2, Batch: 13400, loss: 0.388\n",
      "Epoch: 2, Batch: 13600, loss: 0.358\n",
      "Epoch: 2, Batch: 13800, loss: 0.329\n",
      "Epoch: 2, Batch: 14000, loss: 0.420\n",
      "Epoch: 2, Batch: 14200, loss: 0.379\n",
      "Epoch: 2, Batch: 14400, loss: 0.394\n",
      "Epoch: 2, Batch: 14600, loss: 0.382\n",
      "Epoch: 2, Batch: 14800, loss: 0.428\n",
      "Epoch: 2, Batch: 15000, loss: 0.411\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "overall_step = 0\n",
    "\n",
    "#TODO : Select appropriate number of epochs\n",
    "\n",
    "#Begin Your Code\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "#End Your Code\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        #TODO : Make predictions, calculate accuracy and update your weights once\n",
    "\n",
    "        #Begin Your Code\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #End Your Code\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('Epoch: %d, Batch: %5d, loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "            #Any thing that is added to the \"info\" gets plotted in tensorboard\n",
    "            #TODO : Add the plots in Tensorboard to the report and explain what is happening\n",
    "#             info = { ('loss') : loss.item(),('accuracy'): accuracy.item()}\n",
    "#             for tag, value in info.items():\n",
    "#                 logger.scalar_summary(tag, value, overall_step+1)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RREjngOC_X2F"
   },
   "source": [
    "## 6. Test Accuracy\n",
    "Let us look at how the network performs on the test dataset.  \n",
    "Report your accuracy in your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "#TODO : Report this accuracy in your report.\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCCVNEjV_X2G"
   },
   "source": [
    "## 7. Per Class accuracy\n",
    "Now we see the test accuracy for each class in the test dataset.  \n",
    "Report these accuracies in your report. Also identify the problematic classes.  \n",
    "Can you explain why these classes have significantly lower accuracies compared to other classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top : 87 %\n",
      "Accuracy of Trouser : 95 %\n",
      "Accuracy of Pullover : 80 %\n",
      "Accuracy of Dress : 92 %\n",
      "Accuracy of  Coat : 77 %\n",
      "Accuracy of Sandal : 96 %\n",
      "Accuracy of Shirt : 53 %\n",
      "Accuracy of Sneaker : 93 %\n",
      "Accuracy of   Bag : 97 %\n",
      "Accuracy of Ankle boot : 96 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report of CNN\n",
    "\n",
    "Since increasing layers may increase accuracy of neural network, three fully connected layers are applied here instead of one.\n",
    "The accuracies of each type from top to bottom are 87%, 95%, 80%, 92%, 77%, 96%, 53%, 93%, 97%, 96%, respectively. The total accuracy of the task is 87%.\n",
    "\n",
    "**Shirt** class and have significantly low accuracy because the shirts training set look very different from each other. The weights are not easy to be trained to converge. **Coat** and **pullover** have relatively low accuracy because these two classes are very similar to each other.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}