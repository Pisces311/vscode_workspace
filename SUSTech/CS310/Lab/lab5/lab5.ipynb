{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['此前',\n  '拜登',\n  '政府',\n  '宣布',\n  '将',\n  '投入',\n  '几千',\n  '亿美元',\n  '推动',\n  '美国',\n  '电动汽车',\n  '行业',\n  '发展'],\n ['解放军',\n  '东部',\n  '战区',\n  '新闻',\n  '发言人',\n  '表示',\n  '台湾',\n  '及其',\n  '附属',\n  '岛屿',\n  '是',\n  '中国',\n  '领土',\n  '一部分'],\n ['受', '利好', '消息', '刺激', '影响', '海南', '概念', '板块', '直线', '上行', '券商', '股', '拉升'],\n ['做好', '疫情', '防控', '毫不放松', '抓实', '抓细', '疫情', '防控', '各项措施'],\n ['上海市', '出台', '高中', '招考', '新政', '实验性', '示范性', '高中', '将', '出', '其', '招生', '计划'],\n ['游客',\n  '出游',\n  '热情高涨',\n  '红色旅游',\n  '持续',\n  '升温',\n  '踏青',\n  '游',\n  '近郊',\n  '游',\n  '乡村',\n  '游',\n  '自驾游',\n  '需求',\n  '加速',\n  '释放'],\n ['忙',\n  '新剧',\n  '拍摄',\n  '表示',\n  '完成',\n  '拍摄',\n  '后',\n  '返来',\n  '电视',\n  '城为',\n  '劲歌',\n  '金曲',\n  '担任',\n  '嘉宾',\n  '亦',\n  '很',\n  '开心',\n  '见到',\n  '偶像'],\n ['印度',\n  '菜',\n  '对于',\n  '香港',\n  '人',\n  '并',\n  '不',\n  '陌生',\n  '咖喱',\n  '薄饼',\n  '串烧',\n  '印度',\n  '美食',\n  '更深',\n  '得',\n  '港人',\n  '欢心'],\n ['继上',\n  '一圈',\n  '斗',\n  '巴塞隆',\n  '拿',\n  '两',\n  '回合',\n  '狂',\n  '轰',\n  '四球',\n  '安',\n  '巴比',\n  '这次',\n  '面对',\n  '盟主',\n  '拜仁',\n  '慕尼黑',\n  '又',\n  '有',\n  '佳作']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import jieba\n",
    "\n",
    "text_corpus = [\n",
    "    \"此前拜登政府宣布将投入几千亿美元推动美国电动汽车行业发展\",\n",
    "    \"解放军东部战区新闻发言人表示台湾及其附属岛屿是中国领土的一部分\",\n",
    "    \"受利好消息刺激影响海南概念板块直线上行券商股拉升\",\n",
    "    \"要做好疫情防控毫不放松抓实抓细疫情防控各项措施\",\n",
    "    \"上海市出台高中招考新政实验性示范性高中将出其招生计划\",\n",
    "    \"游客出游热情高涨红色旅游持续升温踏青游近郊游乡村游自驾游等需求加速释放\",\n",
    "    \"忙为新剧拍摄表示完成拍摄后返来电视城为劲歌金曲担任嘉宾亦很开心见到偶像\",\n",
    "    \"印度菜对于香港人并不陌生咖喱薄饼串烧等印度美食更深得港人欢心\",\n",
    "    \"继上一圈斗巴塞隆拿两回合狂轰四球安巴比这次面对盟主拜仁慕尼黑又有佳作\",\n",
    "]\n",
    "\n",
    "stoplist = set('的 要 等 为'.split())\n",
    "\n",
    "texts = [[word for word in jieba.cut(document, cut_all=False) if word not in stoplist] for document in text_corpus]\n",
    "pprint.pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['将'],\n ['表示'],\n [],\n ['疫情', '防控', '疫情', '防控'],\n ['高中', '高中', '将'],\n ['游', '游', '游'],\n ['拍摄', '表示', '拍摄'],\n ['印度', '印度'],\n []]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary(8 unique tokens: ['将', '表示', '疫情', '防控', '高中']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'印度': 7, '将': 0, '拍摄': 6, '游': 5, '疫情': 2, '表示': 1, '防控': 3, '高中': 4}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[(0, 1)],\n [(1, 1)],\n [],\n [(2, 2), (3, 2)],\n [(0, 1), (4, 2)],\n [(5, 3)],\n [(1, 1), (6, 2)],\n [(7, 2)],\n []]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['拍摄', '印度']\n[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.6690057), (7, 0.70710677), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = list(jieba.cut('拍摄印度', cut_all=False))\n",
    "print(query_document)\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}